## Autonomous OSINT Investigation Swarm — Quad Chart (Status Update)

**Date**: 2026-02-28  
**Course/Team**: FSE570 Capstone — Autonomous OSINT Investigation Swarm  
**Goal**: Modular, multi-agent OSINT pipeline for **corporate/entity risk assessment** using **trusted, citable, reproducible** sources.

**Team roster (from Team Charter / Proposal)**:
- Taljinder Singh
- Aditya Pokharna
- Raj Kumar Mahto
- Arnab Mitra
- Jacob Kuriakose

**Working role leads (initial)**:
- **Data gathering & preprocessing**: Taljinder
- **Modeling & backend development**: Arnab, Raj
- **Frontend & visualization**: Aditya
- **Deployment & documentation**: Jacob

| **Latest accomplishments / KPIs** | **Next major tasks (RAIL) / Owners** |
|---|---|
| - **End-to-end vertical slice implemented** (matches capstone proposal): natural-language query → Lead Agent orchestration → specialist agents → Reflexion (cross-check + gap detection + confidence aggregation) → knowledge graph build → outputs (risk dashboard + evidence report + audit trail).<br>- **Quad-layer architecture materialized in code**: `agents/`, `mcp_layer/`, `reflexion_layer/`, `knowledge_graph/`, `output_layer/`, plus runnable scripts and a web UI.<br>- **Evidence-first design**: all agent inputs/outputs are structured `Evidence` (citable claims) + `Entity` schema, enabling audit-ready reporting and uniform downstream processing.<br>- **MCP-style integration layer working**: agents retrieve evidence through a single interface (`get_evidence_for_entity`) with caching under `data/raw/` for reproducibility and traceability (no “direct connector calls” in agent code).<br>- **Authoritative data sources integrated (v1)**:<br>  - **SEC EDGAR submissions JSON** (requires valid `SEC_USER_AGENT`).<br>  - **NHTSA recalls via DOT DataHub** (chosen to avoid 403 failures seen with some legacy endpoints on school networks).<br>- **Web demo working (Flask)**: one-page workflow to run investigation + show entity, tasks, findings counts, risk scores, gaps, conflicts, evidence report, and audit events.<br>- **Proposal execution-timeline implemented conceptually and documented** (decomposition → retrieval → analysis → reflexion), ready to measure next (runtime KPIs).<br>- **Unit test suite** across all major layers (`pytest tests/unit -v`).<br><br>**KPIs to show on the slide (fill before meeting)**:<br>- **Sources integrated**: 2 (SEC + NHTSA).<br>- **Evidence generated (Tesla run)**: ~90 structured evidence rows (generated by scripts; not committed).<br>- **Demo runtime**: _[measure on laptop]_ seconds per run (then compare vs Sol batch runs). | - **Multi-entity demo readiness**: add 1–2 more entities to registry + identifiers (CIK/ticker/make), and ensure the pipeline works for each. **Owners**: Taljinder (data/entity research), Raj (backend wiring)<br><br>- **Replace the highest-impact stubs (to improve “money laundering” coverage)**:<br>  - **Sanctions screening**: integrate an open, citable list such as **OFAC SDN** and implement match logic + evidence emission. **Owners**: Raj (backend), Arnab (scoring/logic)<br>  - **Legal documents**: integrate **CourtListener/RECAP** where available; emit evidence rows with citations + cached PDFs when possible. **Owners**: Jacob (integration + storage), Raj (backend)<br>  - **Beneficial ownership / structure mapping**: integrate **OpenCorporates** (or a reproducible curated dataset if rate-limited) to move beyond the current placeholder. **Owners**: Arnab, Raj<br><br>- **Adverse media (citable & reproducible)**: integrate **GDELT** (or define a small seed corpus + evaluation rubric) and emit evidence with citations. **Owners**: Taljinder (data), Aditya (UI surfacing)<br><br>- **Demo polish for “document-like slide” outputs**: clearer labeling of what is authoritative vs stub, better error reporting, and a one-command run path (CLI + Flask). **Owners**: Aditya (frontend), Jacob (docs/runbooks)<br><br>- **Sol supercomputer plan**: run batch investigations across multiple entities/sources; capture throughput, latency, and caching behavior as KPIs. **Owners**: Jacob (compute + scripts), Arnab/Raj (batch runner + metrics) |
| **Major risks / barriers / obstacles** | **Remaining major activities / plan to complete** |
| - **Data-source access & compliance**: SEC requires valid `User-Agent` and may rate-limit; some sources are paywalled (PACER) or have ToS constraints (social platforms).<br>  - **Mitigation**: already using `.env` + caching; prioritize open/citable sources (SEC, NHTSA, CourtListener, OFAC, GDELT). Treat paywalled sources as “future extensions.”<br>- **Coverage gaps due to stubs**: Legal (sanctions/courts) + Social Graph (network/adverse media) are currently placeholders → risk that the investigation looks “thin” for money-laundering style prompts.<br>  - **Mitigation**: replace sanctions + court docs stubs first (high signal), then add one adverse-media source.<br>- **Entity resolution is registry-based** (currently strong for known entities like Tesla only).<br>  - **Mitigation**: expand curated registry for the course demo; optionally add simple fuzzy matching + identifier-based resolution (CIK/ticker).<br>- **Scoring interpretability**: current risk dashboard aggregates mean confidence by category; this may not represent “severity” or “materiality.”<br>  - **Mitigation**: introduce source reliability weighting + severity heuristics; document scoring policy on the slide (briefly) and in docs (fully).<br>- **Reproducibility vs storage**: caching raw artifacts improves auditability but increases repo/storage risk (already mitigated by gitignore).<br>  - **Mitigation**: keep `data/` ignored; store only `.gitkeep` placeholders; produce run artifacts as local outputs or release bundles if needed.<br>- **Performance & scaling**: more sources/agents can increase latency and cost.<br>  - **Mitigation**: use Sol for batch experiments; keep interactive Flask runs bounded with caching and controlled source sets. | - **Complete the “proposal demo” loop (Phase 4–7 alignment)**:<br>  - Ensure the “money laundering” investigation prompt triggers: corporate structure, beneficial ownership, sanctions, transaction patterns, adverse media — and that at least **sanctions + court docs** are real (not stubbed).<br><br>- **Reflexion pattern (proposal) — implement/validate the 4-step loop**:<br>  - Self-assessment → gap identification → retrieval triggering → convergence check (stop condition).<br>  - Output must show: gaps found, follow-ups suggested/triggered, and confidence changes across iterations.<br><br>- **Knowledge graph & outputs**:<br>  - Persist per-run artifacts (report + audit log + graph summary) in a run directory; keep citations and raw locations stable via caching.<br>  - Add at least one “interactive” or drill-down element (can be simple in Flask: clickable evidence list + filters by risk category/source).<br><br>- **Evaluation plan (from proposal)**:<br>  - Metrics: citations per claim, evidence coverage by category, runtime/latency, completeness (gap count), and comparison vs manual workflow narrative.<br>  - Run multi-entity scenarios and record KPIs (laptop vs Sol).<br><br>**Execution timeline (proposal slide)**:<br>- 0–30s: query decomposition<br>- 30–90s: parallel retrieval (MCP)<br>- 90–180s: analysis & synthesis<br>- 180s+: reflexion & refinement<br><br>**Near-term milestone schedule (adjust to course calendar)**:<br>- Next 1–2 weeks: OFAC + CourtListener integrated; multi-entity demo; record runtime + evidence KPIs.<br>- Following 1–2 weeks: add adverse media source + structure mapping; improve scoring and reporting UI.<br>- Final weeks: Sol batch runs + evaluation write-up + final demo polish + deployment runbook. |

**Presenter assignment (one quadrant each)**:
- Top-left (Accomplishments/KPIs): **Arnab Mitra**
- Top-right (Next tasks/RAIL): **Taljinder Singh**
- Bottom-left (Risks/Barriers): **Jacob Kuriakose**
- Bottom-right (Remaining plan/timeline): **Raj Kumar Mahto**

**Additional support role (non-quadrant, optional)**:
- **Metrics & demo operator / Q&A**: **Aditya Pokharna** (runs the demo live, captures KPIs, answers technical questions)

