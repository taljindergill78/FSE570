## Autonomous OSINT Investigation Swarm — Quad Chart (Status Update)

**Date**: 2026-02-28  
**Course/Team**: FSE570 Capstone — Autonomous OSINT Investigation Swarm  
**Goal**: Modular, multi-agent OSINT pipeline for **corporate/entity risk assessment** using **trusted, citable, reproducible** sources.

| **Latest accomplishments / KPIs** | **Next major tasks (RAIL) / Owners** |
|---|---|
| - **End-to-end pipeline runs**: Lead Agent → specialist agents → reflexion (gaps/conflicts/confidence) → knowledge graph → outputs (risk dashboard + evidence report + audit trail).<br>- **MCP-style data layer implemented**: unified `get_evidence_for_entity()` interface with caching under `data/raw/`.<br>- **Core sources wired (v1)**: **SEC EDGAR submissions JSON** + **NHTSA recalls via DOT DataHub** (chosen to avoid 403 issues from legacy endpoints).<br>- **Canonical schema defined**: `Entity` + `Evidence` (evidence is the standard “claim” unit all agents consume/emit).<br>- **Web demo working (Flask)**: browser form runs full pipeline and renders tasks, findings counts, risk scores, gaps/conflicts, report, audit events.<br>- **Reproducible vertical slice scripts**: `pull_sec_submissions.py`, `pull_nhtsa_recalls.py`, `build_evidence_tesla.py`, `run_lead_agent.py` (Tesla).<br>- **Tests in place**: unit tests across agents, MCP layer, reflexion, knowledge graph, output layer (`pytest tests/unit -v`).<br><br>**KPI candidates (this week)**:<br>- Sources integrated: **2 authoritative sources** (SEC + NHTSA).<br>- Vertical-slice evidence: **~90 structured evidence rows** for Tesla (SEC + NHTSA) when running the provided scripts (see repo README). | - **Expand beyond Tesla**: add 1–2 more entities in `ENTITY_REGISTRY` + identifiers (CIK/ticker/make) so the demo supports multiple companies. **Owner**: _[Name]_<br>- **Replace key stubs (highest value)**:<br>  - Sanctions screening: ingest **OFAC SDN** (or other open sanctions list) into Legal Agent. **Owner**: _[Name]_<br>  - Legal docs: integrate **CourtListener/RECAP** where available (free). **Owner**: _[Name]_<br>  - Beneficial ownership / structure mapping: integrate **OpenCorporates** (or a curated sample dataset if rate-limited). **Owner**: _[Name]_<br>- **Adverse media**: integrate a citable open source (e.g., **GDELT**) or define a constrained “seed corpus” for reproducible evaluation. **Owner**: _[Name]_<br>- **Hardening / demo polish**: better error surfacing in Flask UI, clearer “what’s stubbed vs real,” and packaged run instructions. **Owner**: _[Name]_<br>- **Scale run on Sol**: batch-run investigations across multiple entities; capture runtime + throughput metrics. **Owner**: _[Name]_ |
| **Major risks / barriers / obstacles** | **Remaining major activities / plan to complete** |
| - **Source access constraints**: SEC requires valid `User-Agent`; some endpoints rate-limit. NHTSA legacy endpoints may 403 on school networks.<br>  - **Mitigation**: use `.env` for SEC UA, caching to `data/raw/`, and DOT DataHub NHTSA ingestion already implemented.<br>- **Coverage gaps (stubs)**: Legal Agent (sanctions/PACER) + Social Graph Agent (network/adverse media) are currently stubbed → risk of thin “money laundering” coverage.<br>  - **Mitigation**: prioritize OFAC + CourtListener + one adverse-media source next.<br>- **Entity resolution is registry-based**: currently reliable for known entities only (e.g., Tesla).<br>  - **Mitigation**: add lightweight identifier lookup + fuzzy matching; keep a small curated roster for class demos.<br>- **Evidence quality & scoring**: confidence scoring is simple; risk dashboard uses mean confidence by category → may not reflect “severity.”<br>  - **Mitigation**: add source-reliability weighting and severity heuristics; document assumptions.<br>- **Time/compute**: adding more sources may increase latency and complexity.<br>  - **Mitigation**: use Sol for batch workloads; keep interactive demo bounded with caching. | - **Close Phase 4 (Specialists)**: implement at least 2 “real” specialists end-to-end (Legal: sanctions + court docs; Social/Media: adverse media) so the pipeline isn’t mostly stub outputs for non-corporate tasks.<br>- **Phase 5 (Reflexion)**: extend gap detection + conflict detection to be source-aware; calibrate confidence aggregation and document scoring policy.<br>- **Phase 6 (Knowledge graph + outputs)**: persist graph/report artifacts per run; add a lightweight visualization (even a summary table/graph stats is acceptable if kept reproducible/citable).<br>- **Phase 7 (Demo + evaluation)**: define success metrics (coverage, citations per claim, runtime), run multi-entity scenarios, and produce a final “one-command” demo path (CLI + Flask).<br><br>**Proposed mini-timeline (adjust to your course dates)**:<br>- **Next 1–2 weeks**: replace sanctions + court docs stubs; add 1 more entity; record KPIs (runtime, evidence count, citations).<br>- **Following 1–2 weeks**: add adverse media + structure mapping; improve scoring; improve UI/report output.<br>- **Final weeks**: Sol batch runs + evaluation writeup + final demo polish. |

**Presenter assignment (one quadrant each)**:
- Top-left (Accomplishments/KPIs): _Arnab Mitra_
- Top-right (Next tasks/RAIL): _Taljinder Singh_
- Bottom-left (Risks/Barriers): _[Team member 3]_
- Bottom-right (Remaining plan/timeline): _[Team member 4]_

